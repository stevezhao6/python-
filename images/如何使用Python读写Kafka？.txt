如<div class="post-body" itemprop="articleBody">

      
      

      
        <p>关于Kafka的第三篇文章，我们来讲讲如何使用Python读写Kafka。这一篇文章里面，我们要使用的一个第三方库叫做<code>kafka-python</code>。大家可以使用<code>pip</code>或者<code>pipenv</code>安装它。下面两种安装方案，任选其一即可。</p>
<a id="more"/>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/></pre></td><td class="code"><pre><span class="line">python3 -m pip install kafka-python</span><br/><span class="line">pipenv install kafka-python</span><br/></pre></td></tr></table></figure>
<p>如下图所示：</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/2019-12-17-20-07-03.png" alt=""/></p>
<p>这篇文章，我们将会使用最短的代码来实现一个读、写Kafka的示例。</p>
<h2 id="创建配置文件"><a href="#创建配置文件" class="headerlink" title="创建配置文件"/>创建配置文件</h2><p>由于生产者和消费者都需要连接Kafka，所以我单独写了一个配置文件<code>config.py</code>用来保存连接Kafka所需要的各个参数，而不是直接把这些参数Hard Code写在代码里面：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/></pre></td><td class="code"><pre><span class="line"><span class="comment"># config.py</span></span><br/><span class="line">SERVER = <span class="string">'123.45.32.11:1234'</span></span><br/><span class="line">USERNAME = <span class="string">'kingname'</span></span><br/><span class="line">PASSWORD = <span class="string">'kingnameisgod'</span></span><br/><span class="line">TOPIC = <span class="string">'howtousekafka'</span></span><br/></pre></td></tr></table></figure>
<p>本文演示所用的Kafka由我司平台组的同事搭建，需要账号密码才能连接，所以我在配置文件中加上了<code>USERNAME</code>和<code>PASSWORD</code>两项。你使用的Kafka如果没有账号和密码，那么你只需要<code>SERVER</code>和<code>TOPIC</code>即可。</p>
<h2 id="创建生产者"><a href="#创建生产者" class="headerlink" title="创建生产者"/>创建生产者</h2><p>代码简单到甚至不需要解释。首先使用<code>KafkaProducer</code>类连接 Kafka，获得一个生产者对象，然后往里面写数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br/><span class="line"><span class="keyword">import</span> time</span><br/><span class="line"><span class="keyword">import</span> datetime</span><br/><span class="line"><span class="keyword">import</span> config</span><br/><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaProducer</span><br/><span class="line"/><br/><span class="line"/><br/><span class="line">producer = KafkaProducer(bootstrap_servers=config.SERVER,</span><br/><span class="line">                         value_serializer=<span class="keyword">lambda</span> m: json.dumps(m).encode())</span><br/><span class="line"/><br/><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br/><span class="line">    data = {<span class="string">'num'</span>: i, <span class="string">'ts'</span>: datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)}</span><br/><span class="line">    producer.send(config.TOPIC, data)</span><br/><span class="line">    time.sleep(<span class="number">1</span>)</span><br/></pre></td></tr></table></figure>
<p>参数<code>bootstrap_servers</code>用于指定 Kafka 的服务器连接地址。</p>
<p>参数<code>value_serializer</code>用来指定序列化的方式。这里我使用 json 来序列化数据，从而实现我向 Kafka 传入一个字典，Kafka 自动把它转成 JSON 字符串的效果。</p>
<p>如下图所示：</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/2019-12-17-20-40-18.png" alt=""/></p>
<p>注意，上图中，我多写了4个参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/></pre></td><td class="code"><pre><span class="line">security_protocol=<span class="string">"SASL_PLAINTEXT"</span></span><br/><span class="line">sasl_mechanism=<span class="string">"PLAIN"</span></span><br/><span class="line">sasl_plain_username=config.USERNAME</span><br/><span class="line">sasl_plain_password=config.PASSWORD</span><br/></pre></td></tr></table></figure>
<p>这四个参数是因为我这里需要通过密码连接 Kafka 而加上的，如果你的 Kafka 没有账号密码，就不需要这四个参数。</p>
<h2 id="创建消费者"><a href="#创建消费者" class="headerlink" title="创建消费者"/>创建消费者</h2><p>Kafka 消费者也需要连接 Kafka，首先使用<code>KafkaConsumer</code>类初始化一个消费者对象，然后循环读取数据。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> config</span><br/><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaConsumer</span><br/><span class="line"/><br/><span class="line"/><br/><span class="line">consumer = KafkaConsumer(config.TOPIC,</span><br/><span class="line">                         bootstrap_servers=config.SERVER,</span><br/><span class="line">                         group_id=<span class="string">'test'</span>,</span><br/><span class="line">                         auto_offset_reset=<span class="string">'earliest'</span>)</span><br/><span class="line"><span class="keyword">for</span> msg <span class="keyword">in</span> consumer:</span><br/><span class="line">    print(msg.value)</span><br/></pre></td></tr></table></figure>
<p>KafkaConsumer 的第一个参数用于指定 Topic。你可以把这个 Topic 理解成 Redis 的 Key。</p>
<p>bootstrap_servers用于指定 Kafka 服务器连接地址。</p>
<p>group_id这个参数后面的字符串可以任意填写。如果两个程序的<code>Topic</code>与<code>group_id</code>相同，那么他们读取的数据不会重复，两个程序的<code>Topic</code>相同，但是<code>group_id</code>不同，那么他们各自消费全部数据，互不影响。</p>
<p>auto_offset_rest 这个参数有两个值，<code>earliest</code>和<code>latest</code>，如果省略这个参数，那么默认就是<code>latest</code>。这个参数会单独介绍。这里先略过。</p>
<p>连接好 Kafka 以后，直接对消费者对象使用 for 循环迭代，就能持续不断获取里面的数据了。</p>
<h2 id="运行演示"><a href="#运行演示" class="headerlink" title="运行演示"/>运行演示</h2><p>运行两个消费者程序和一个生产者程序，效果如下图所示。</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/2019-12-17-20-44-26.png" alt=""/></p>
<p>我们可以看到，两个消费者程序读取数据不重复，不遗漏。</p>
<p>当所有数据都消费完成以后，如果你把两个消费者程序关闭，再运行其中一个，你会发现已经没有数据会被打印出来了。</p>
<p>但如果你修改一下 group_id，程序又能正常从头开始消费了，如下图所示：</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/2019-12-17-20-48-55.png" alt=""/></p>
<h2 id="很多人都会搞混的几个地方"><a href="#很多人都会搞混的几个地方" class="headerlink" title="很多人都会搞混的几个地方"/>很多人都会搞混的几个地方</h2><h3 id="earliest-与-latest"><a href="#earliest-与-latest" class="headerlink" title="earliest 与 latest"/>earliest 与 latest</h3><p>在我们创建消费者对象的时候，有一个参数叫做<code>auto_offset_reset='earliest'</code>。有人看到<code>earliest</code>与<code>latest</code>，想当然地认为设置为<code>earliest</code>，就是从 Topic 的头往后读，设置为<code>latest</code>就是忽略之前的数据，从程序运行以后，新来的数据开始读。</p>
<p>这种看法是不正确的。</p>
<p><code>auto_offset_reset</code>这个参数，只有在一个<code>group</code>第一次运行的时候才有作用，从第二次运行开始，这个参数就失效了。</p>
<p>假设现在你的 Topic 里面有100个数据，你设置了一个全新的 group_id 为<code>test2</code>。<code>auto_offset_reset</code>设置为 <code>earliest</code>。那么当你的消费者运行的时候，Kafka 会先把你的 offset 设置为0，然后让你从头开始消费的。</p>
<p>假设现在你的 Topic 里面有100个数据，你设置了一个全新的 group_id 为<code>test3</code>。<code>auto_offset_reset</code>设置为 <code>latest</code>。那么当你的消费者运行的时候，Kafka 不会给你返回任何数据，消费者看起来就像卡住了一样，但是 Kafka 会直接强制把前100条数据的状态设置为已经被你消费的状态。所以当前你的 offset 就直接是99了。直到生产者插入了一条新的数据，此时消费者才能读取到。这条新的数据对应的 offset 就变成了100。</p>
<p>假设现在你的 Topic 里面有100个数据，你设置了一个全新的 group_id 为<code>test4</code>。<code>auto_offset_reset</code>设置为 <code>earliest</code>。那么当你的消费者运行的时候，Kafka 会先把你的 offset 设置为0，然后让你从头开始消费的。等消费到第50条数据时，你把消费者程序关了，把<code>auto_offset_reset</code>设置为<code>latest</code>，再重新运行。此时消费者依然会接着从第51条数据开始读取。不会跳过剩下的50条数据。</p>
<p>所以，auto_offset_reset的作用，是在你的 group 第一次运行，还没有 offset 的时候，给你设定初始的 offset。而一旦你这个 group 已经有 offset 了，那么auto_offset_reset这个参数就不会再起作用了。</p>
<h3 id="partition-是如何分配的？"><a href="#partition-是如何分配的？" class="headerlink" title="partition 是如何分配的？"/>partition 是如何分配的？</h3><p>对于同一个 Topic 的同一个 Group：</p>
<p>假设你的 Topic 有10个  Partition，一开始你只启动了1个消费者。那么这个消费者会轮换着从这10个Partition 中读取数据。</p>
<p>当你启动第二个消费者时，Kafka 会从第一个消费者手上抢走5个Partition，分给第二个消费者。于是两个消费者各自读5个 Partition。互不影响。</p>
<p>当第三个消费者又出现时，Kafka 从第一个消费者手上再抢走1个 Partition，从第二个消费者手上抢走2个 Partition 给第三个消费者。于是，消费者1有4个 Partition，消费者2有3个 Partition，消费者3有3个 Partiton，互不影响。</p>
<p>当你有10个消费者一起消费时，每个消费者读取一个 Partition，互不影响。</p>
<p>当第11个消费者出现时，它由于分配不到 Partition，所以它什么都读不到。</p>
<p>所以在上一篇文章中，我说，在同一个 Topic，同一个 Group 中，你有多少个 Partiton，就能起多少个进程同时消费。</p>
<h3 id="Kafka-是不是完全不重复不遗漏？"><a href="#Kafka-是不是完全不重复不遗漏？" class="headerlink" title="Kafka 是不是完全不重复不遗漏？"/>Kafka 是不是完全不重复不遗漏？</h3><p>在极端情况下，Kafka 会重复，也会遗漏，但是这种极端情况并不常见。如果你的 Kafka 频繁漏数据，或者总是出现重复数据，那么肯定是你环境没有搭建正确，或者代码有问题。</p>
<h3 id="忠告"><a href="#忠告" class="headerlink" title="忠告"/>忠告</h3><p>再次提醒：专业的人做专业的事情，不要轻易自建Kafka 集群。让专门的同事复制搭建和维护，你只管使用。这才是最高效省事的做法。</p>

      
    </div>

    

    
    
    

    
      