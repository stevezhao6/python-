为<div class="post-body" itemprop="articleBody">

      
      

      
        <p>听说过异步爬虫的同学，应该或多或少听说过<code>aiohttp</code>这个库。它通过 Python 自带的<code>async/await</code>实现了异步爬虫。</p>
<p>使用 aiohttp，我们可以通过 requests 的api写出并发量匹敌 Scrapy 的爬虫。</p>
<a id="more"/>
<p>我们在 aiohttp 的官方文档上面，可以看到它给出了一个代码示例，如下图所示：</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/2019-12-18-21-55-56.png" alt=""/></p>
<p>我们现在稍稍修改一下，来看看这样写爬虫，运行效率如何。</p>
<p>修改以后的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br/><span class="line"><span class="keyword">import</span> aiohttp</span><br/><span class="line"/><br/><span class="line">template = <span class="string">'http://exercise.kingname.info/exercise_middleware_ip/{page}'</span></span><br/><span class="line"/><br/><span class="line"/><br/><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(session, page)</span>:</span></span><br/><span class="line">    url = template.format(page=page)</span><br/><span class="line">    resp = <span class="keyword">await</span> session.get(url)</span><br/><span class="line">    print(<span class="keyword">await</span> resp.text(encoding=<span class="string">'utf-8'</span>))</span><br/><span class="line"/><br/><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br/><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br/><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">100</span>):</span><br/><span class="line">            <span class="keyword">await</span> get(session, page)</span><br/><span class="line"/><br/><span class="line"/><br/><span class="line">loop = asyncio.get_event_loop()</span><br/><span class="line">loop.run_until_complete(main())</span><br/></pre></td></tr></table></figure>
<p>这段代码访问我的爬虫练习站100次，获取100页的内容。</p>
<p>大家可以通过下面这个视频看看它的运行效率：</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/slow.2019-12-18 22_51_37.gif" alt=""/></p>
<p>可以说，目前这个运行速度，跟 requests 写的单线程爬虫几乎没有区别，代码还多了那么多。</p>
<p>那么，应该如何正确释放 aiohttp 的超能力呢？</p>
<p>我们现在把代码做一下修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br/><span class="line"><span class="keyword">import</span> aiohttp</span><br/><span class="line"/><br/><span class="line">template = <span class="string">'http://exercise.kingname.info/exercise_middleware_ip/{page}'</span></span><br/><span class="line"/><br/><span class="line"/><br/><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(session, queue)</span>:</span></span><br/><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br/><span class="line">        <span class="keyword">try</span>:</span><br/><span class="line">            page = queue.get_nowait()</span><br/><span class="line">        <span class="keyword">except</span> asyncio.QueueEmpty:</span><br/><span class="line">            <span class="keyword">return</span></span><br/><span class="line">        url = template.format(page=page)</span><br/><span class="line">        resp = <span class="keyword">await</span> session.get(url)</span><br/><span class="line">        print(<span class="keyword">await</span> resp.text(encoding=<span class="string">'utf-8'</span>))</span><br/><span class="line"/><br/><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br/><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br/><span class="line">        queue = asyncio.Queue()</span><br/><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br/><span class="line">            queue.put_nowait(page)</span><br/><span class="line">        tasks = []</span><br/><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br/><span class="line">            task = get(session, queue)</span><br/><span class="line">            tasks.append(task)</span><br/><span class="line">        <span class="keyword">await</span> asyncio.wait(tasks)</span><br/><span class="line"/><br/><span class="line"/><br/><span class="line">loop = asyncio.get_event_loop()</span><br/><span class="line">loop.run_until_complete(main())</span><br/></pre></td></tr></table></figure>
<p>在修改以后的代码里面，我让这个爬虫爬1000页的内容，我们来看看下面这个视频。</p>
<p><img src="https://kingname-1257411235.cos.ap-chengdu.myqcloud.com/fast.2019-12-18 22_49_49.gif" alt=""/></p>
<p>可以看到，目前这个速度已经可以跟 Scrapy 比一比了。并且大家需要知道，这个爬虫只有1个进程1个线程，它是通过异步的方式达到这个速度的。</p>
<p>那么，修改以后的代码，为什么速度能快那么多呢？</p>
<p>关键的代码，就在：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/></pre></td><td class="code"><pre><span class="line">tasks = []</span><br/><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br/><span class="line">    task = get(session, queue)</span><br/><span class="line">    tasks.append(task)</span><br/><span class="line"><span class="keyword">await</span> asyncio.wait(tasks)</span><br/></pre></td></tr></table></figure>
<p>在慢速版本里面，我们只有1个协程在运行。而在现在这个快速版本里面，我们创建了100个协程，并把它提交给<code>asyncio.wait</code>来统一调度。<code>asyncio.wait</code>会在所有协程全部结束的时候才返回。</p>
<p>我们把1000个 URL 放在<code>asyncio.Queue</code>生成的一个异步队列里面，每一个协程都通过 while True 不停从这个异步队列里面取 URL 并进行访问，直到异步队列为空，退出。</p>
<p>当程序运行时，Python 会自动调度这100个协程，当一个协程在等待网络 IO 返回时，切换到第二个协程并发起请求，在这个协程等待返回时，继续切换到第三个协程并发起请求……。程序充分利用了网络 IO 的等待时间，从而大大提高了运行速度。</p>
<p>最后，感谢实习生小河给出的这种加速方案。</p>

      
    </div>

    

    
    
    

    
      